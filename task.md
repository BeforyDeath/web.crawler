Задание делается за 16 часов.
Сразу скажу: сделать его в полном объёме (со всей обработкой ошибок, corner cases, тестами прочим) за 16 часов нереально совершенно.
Мы ходим увидеть, как вы по постановке задаче самостоятельно примете решение, что более важно, а что менее.
На выходе мы хотим получить работающее приложение.
Не сидите над заданием больше 16 часов, поставьте отсечку good enough.

Задача:
- Реализовать web-crawler, рекурсивно скачивающий сайт (идущий по ссылкам вглубь). Crawler должен скачать документ по указанному URL и продолжить закачку по ссылкам, находящимся в документе.
- Crawler должен поддерживать дозакачку.
- Crawler должен сжимать скачанные документы перед сохранением  (gzip)
- Crawler должен грузить только текстовые документы -   html, css, js (картинки, видео игнорировать)
- Crawler должен грузить документы только одного домена (игнорировать сторонние ссылки)

За указанное время вы успеете реализовать лишь грубый прототип, который, скорей всего, не будет хорошо обрабатывать ошибки.
Это не беда!
Цели, напоминаю:
- посмотреть, как вы умеете работать с требованиями
- посмотреть, как именно вы расставляете приоритеты
- посмотреть, как вы умеете проектировать приложения
- посмотреть, насколько хорошо вы умеете перекладывать принятые вами решения на конечный язык программирования  (golang)

10.45 start
13.15 stop

